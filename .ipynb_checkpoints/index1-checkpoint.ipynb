{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Review - Predictive Classification Workflow\n",
    "\n",
    "## Students Will Be Able To\n",
    "- Understand the overall process to solve a predictive classification problem\n",
    "- Understand and implement multiple classification algorithms\n",
    "- Implement cross-validation techniques\n",
    "- Handle class imbalance using SMOTE\n",
    "- Use Regularization to perform feature selection\n",
    "- Perform GridSearch to determine optimal hyperparameter combinations\n",
    "- Create Pipelines to streamline the modeling process\n",
    "\n",
    "## Business and Data Understanding\n",
    "\n",
    "This dataset was downloaded from [Kaggle](https://www.kaggle.com/uciml/adult-census-income) and contains information on adult incomes. We are trying to predict whether or not an individual's yearly salary was greater than or equal to \\$ 50,000 (binary classification). The column `salary` will be either a 0 (less than \\$ 50,000) or a 1 (greater than or equal to \\$50,000). The metric we will be using is accuracy.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "#### Train-Test Split\n",
    "\n",
    "We will be using cross-validation for the duration of this notebook. Please perform two train-test splits. First splitting the entire dataframe into train and test sets and then splitting the *train* data into *training and validation sets. Use `random_state=2021` and `test_size=.15` in both splits for reproducibility. We will be using the train and validation sets for the majority of this notebook. **The test set should be left alone until the very end**.\n",
    "\n",
    "#### Preprocessing\n",
    "\n",
    "Please perform the standard data preprocessing steps on the training and validation data:\n",
    "- Check for missing data and impute if necessary (or drop)\n",
    "- Scale numerical data\n",
    "- OneHotEncode categorical data\n",
    "\n",
    "### Modeling\n",
    "\n",
    "#### Baseline Logistic Regression\n",
    "\n",
    "Create a `LogisticRegression` model and fit it on the preprocessed training data. Check the performance of the model on the training and validation data. \n",
    "\n",
    "Please plot a confusion matrix of the model's predictions and compare it to the previous performance metric. What might be causing the accuracy score to be misleading? (*HINT*: Check the value counts of your target variable)\n",
    "\n",
    "#### Second Logistic Regression\n",
    "\n",
    "Please use SMOTE ([documentation here](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)) to adjust the imbalance of target classes. You can use your preprocessed training data at this step. Once you have resampled your training data, please fit another Logistic Regression model and check its performance using the training and validation data. Plot another confusion matrix and explain whether or not resampling helped improve the performance of your model.\n",
    "\n",
    "#### Third LogisticRegression\n",
    "\n",
    "Please create a third and final LogisticRegression model and adjust at least one hyperparameter related to the regularization of the model. Fit the model on the preprocessed, resampled training data. Check the performance on the training and validation data.\n",
    "\n",
    "Inspect the coefficients of this third model and report the 5 features with the largest coefficients and the 5 features with the lowest coefficients. ([documentaion here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html))\n",
    "\n",
    "#### DecisionTreeClassifier\n",
    "\n",
    "Create a `DecisionTreeClassifier` using hyperparameters of your choosing. Please fit the model on the training data and check its performance on the train and validation sets.\n",
    "\n",
    "#### GridSearch RandomForest\n",
    "\n",
    "For your final model, please use `GridSearchCV` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)) to determine the optimal hyperparameter combination for a `RandomForestClassifier`\n",
    "\n",
    "Data has been imported for you. Please perform the following steps\n",
    "\n",
    "- Preprocess\n",
    "    - Train-validation-test split\n",
    "    - Impute missing values\n",
    "    - Scale numerical features\n",
    "    - OHE categorical\n",
    "- Basic Logistic Regression model\n",
    "    - Show features with the 5 highest and 5 lowest coefficient values\n",
    "- Use SMOTE to resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (16281, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   workclass      education  education-num       marital-status  \\\n",
       "0   25     Private           11th              7        Never-married   \n",
       "1   38     Private        HS-grad              9   Married-civ-spouse   \n",
       "2   28   Local-gov     Assoc-acdm             12   Married-civ-spouse   \n",
       "3   44     Private   Some-college             10   Married-civ-spouse   \n",
       "4   18         NaN   Some-college             10        Never-married   \n",
       "\n",
       "           occupation relationship    race      sex  capital-gain  \\\n",
       "0   Machine-op-inspct    Own-child   Black     Male             0   \n",
       "1     Farming-fishing      Husband   White     Male             0   \n",
       "2     Protective-serv      Husband   White     Male             0   \n",
       "3   Machine-op-inspct      Husband   Black     Male          7688   \n",
       "4                 NaN    Own-child   White   Female             0   \n",
       "\n",
       "   capital-loss  hours-per-week  native-country  salary  \n",
       "0             0              40   United-States       0  \n",
       "1             0              50   United-States       0  \n",
       "2             0              40   United-States       1  \n",
       "3             0              40   United-States       1  \n",
       "4             0              30   United-States       0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read data into dataframe and remove whitespace from columns\n",
    "df = pd.read_csv('data/adult.csv')\n",
    "df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "# Replace '?' with np.nan\n",
    "df = df.replace(' ?', np.nan)\n",
    "\n",
    "# Convert salary column to 0 and 1\n",
    "df['salary'] = [1 if s==' >50K.' else 0 for s in df['salary']]\n",
    "\n",
    "# Drop fnlwgt column\n",
    "df = df.drop('fnlwgt', axis=1)\n",
    "\n",
    "# Print shape and head\n",
    "print('Shape: ', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a Metric\n",
    "\n",
    "In the cell below, please indicate which classification metric you think we should use by assigning it to the variable `metric`. Remember we want to *minimize false positives*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "metric = 'precision'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector, make_column_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Before performing any preprocesing or modeling, we need to perform a train-test split. We will cover two forms of cross-validation in this notebook, the second being one you are already familiar with: `cross_val_score`. \n",
    "\n",
    "For now, we conduct a more manual cross-validation by performing a double train-test split. The first will split the data into training and testing sets, the second will split the *training data* into a training and *validation set*. **We will not touch the test set until we have decided upon a final model.**\n",
    "\n",
    "In the cell below, separate your features and target variable and perform the first train-test split. Please set `random_state=2021` and `test_size=.15`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target and features\n",
    "X = df.drop('salary', axis=1)\n",
    "y = df['salary']\n",
    "\n",
    "# Initial train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15, random_state=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create a *validation* set. \n",
    "\n",
    "In the cell below, please perform your second data split using `X_train` and `y_train` and setting `random_state=2021` and `test_size=.15`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation split\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train, test_size=.15, random_state=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We have successfully split our data, the next step is to prepare it for modeling. \n",
    "\n",
    "Please perform the following steps:\n",
    "- Check for missing values and impute if necessary (or drop)\n",
    "- Scale the numerical data\n",
    "- OneHotEncode categorical data\n",
    "\n",
    "For the sake of this example, use `select_dtypes()` to determine what is numeric and what is categorical.\n",
    "\n",
    "Pay attention to the data types of the missing columns. You may need to adjust your imputer `strategy`.\n",
    "\n",
    "At this time, please **do not touch `X_test` and `y_test`**. You should use `X_val` and `y_val` in their place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions:\n",
    "\n",
    "- Impute\n",
    "- Scale\n",
    "- OHE\n",
    "- Combine\n",
    "- data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 0\n",
       "workclass         691\n",
       "fnlwgt              0\n",
       "education           0\n",
       "education-num       0\n",
       "marital-status      0\n",
       "occupation        694\n",
       "relationship        0\n",
       "race                0\n",
       "sex                 0\n",
       "capital-gain        0\n",
       "capital-loss        0\n",
       "hours-per-week      0\n",
       "native-country    198\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 0\n",
       "workclass         129\n",
       "fnlwgt              0\n",
       "education           0\n",
       "education-num       0\n",
       "marital-status      0\n",
       "occupation        129\n",
       "relationship        0\n",
       "race                0\n",
       "sex                 0\n",
       "capital-gain        0\n",
       "capital-loss        0\n",
       "hours-per-week      0\n",
       "native-country     32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_t` and `X_val` are missing values from the same column. Below I create a SimpleImputer and impute the missing values in each. \n",
    "\n",
    "Pay attention to the data types of the missing columns. You may need to adjust your imputer `strategy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SimpleImputer\n",
    "imputer = SimpleImputer(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on X_t\n",
    "imputer.fit(X_t)\n",
    "\n",
    "# Transform X_t and X_val\n",
    "X_t_imputed = pd.DataFrame(imputer.transform(X_t), columns=X_t.columns)\n",
    "X_val_imputed = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Numeric and Categorical data\n",
    "numeric = X.select_dtypes('number')\n",
    "categorical = X.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t_numeric = X_t_imputed[numeric.columns]\n",
    "X_t_categorical = X_t_imputed[categorical.columns]\n",
    "\n",
    "X_val_numeric = X_val_imputed[numeric.columns]\n",
    "X_val_categorical = X_val_imputed[categorical.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Scaler on numeric X_t\n",
    "scaler.fit(X_t_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform numeric X_t and X_val\n",
    "X_t_scaled = pd.DataFrame(scaler.transform(X_t_numeric), columns=numeric.columns)\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(X_val_numeric), columns=numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate OHE\n",
    "ohe = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(sparse=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit on categorical data\n",
    "ohe.fit(X_t_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_t and X_val categorical\n",
    "X_t_encoded = pd.DataFrame(ohe.transform(X_t_categorical), columns=ohe.get_feature_names())\n",
    "X_val_encoded = pd.DataFrame(ohe.transform(X_val_categorical), columns=ohe.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t_df = pd.concat([X_t_scaled, X_t_encoded], axis=1)\n",
    "X_val_df = pd.concat([X_val_scaled, X_val_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                    float64\n",
       "fnlwgt                 float64\n",
       "education-num          float64\n",
       "capital-gain           float64\n",
       "capital-loss           float64\n",
       "                        ...   \n",
       "x7_ Thailand           float64\n",
       "x7_ Trinadad&Tobago    float64\n",
       "x7_ United-States      float64\n",
       "x7_ Vietnam            float64\n",
       "x7_ Yugoslavia         float64\n",
       "Length: 104, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>x0_ Federal-gov</th>\n",
       "      <th>x0_ Local-gov</th>\n",
       "      <th>x0_ Never-worked</th>\n",
       "      <th>x0_ Private</th>\n",
       "      <th>...</th>\n",
       "      <th>x7_ Portugal</th>\n",
       "      <th>x7_ Puerto-Rico</th>\n",
       "      <th>x7_ Scotland</th>\n",
       "      <th>x7_ South</th>\n",
       "      <th>x7_ Taiwan</th>\n",
       "      <th>x7_ Thailand</th>\n",
       "      <th>x7_ Trinadad&amp;Tobago</th>\n",
       "      <th>x7_ United-States</th>\n",
       "      <th>x7_ Vietnam</th>\n",
       "      <th>x7_ Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.492522</td>\n",
       "      <td>0.969261</td>\n",
       "      <td>-0.029882</td>\n",
       "      <td>-0.14322</td>\n",
       "      <td>-0.219136</td>\n",
       "      <td>-0.429349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.067515</td>\n",
       "      <td>2.016254</td>\n",
       "      <td>-0.029882</td>\n",
       "      <td>-0.14322</td>\n",
       "      <td>-0.219136</td>\n",
       "      <td>0.609927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.519951</td>\n",
       "      <td>-0.718296</td>\n",
       "      <td>1.130096</td>\n",
       "      <td>-0.14322</td>\n",
       "      <td>-0.219136</td>\n",
       "      <td>0.769816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.082470</td>\n",
       "      <td>-0.336004</td>\n",
       "      <td>-0.416541</td>\n",
       "      <td>-0.14322</td>\n",
       "      <td>-0.219136</td>\n",
       "      <td>0.370094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.205026</td>\n",
       "      <td>0.013635</td>\n",
       "      <td>-0.029882</td>\n",
       "      <td>-0.14322</td>\n",
       "      <td>-0.219136</td>\n",
       "      <td>-0.189516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>-0.276900</td>\n",
       "      <td>-0.470936</td>\n",
       "      <td>-0.416541</td>\n",
       "      <td>-0.14322</td>\n",
       "      <td>-0.219136</td>\n",
       "      <td>-0.909015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>-0.492522</td>\n",
       "      <td>-0.163156</td>\n",
       "      <td>-0.029882</td>\n",
       "      <td>-0.14322</td>\n",
       "      <td>-0.219136</td>\n",
       "      <td>-0.029627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>-0.276900</td>\n",
       "      <td>-1.514995</td>\n",
       "      <td>-1.189860</td>\n",
       "      <td>-0.14322</td>\n",
       "      <td>-0.219136</td>\n",
       "      <td>-2.747736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>0.801210</td>\n",
       "      <td>0.037913</td>\n",
       "      <td>1.130096</td>\n",
       "      <td>-0.14322</td>\n",
       "      <td>3.911634</td>\n",
       "      <td>0.609927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>-1.498759</td>\n",
       "      <td>1.262771</td>\n",
       "      <td>-0.029882</td>\n",
       "      <td>-0.14322</td>\n",
       "      <td>-0.219136</td>\n",
       "      <td>-2.427958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2076 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "0    -0.492522  0.969261      -0.029882      -0.14322     -0.219136   \n",
       "1    -1.067515  2.016254      -0.029882      -0.14322     -0.219136   \n",
       "2     1.519951 -0.718296       1.130096      -0.14322     -0.219136   \n",
       "3     0.082470 -0.336004      -0.416541      -0.14322     -0.219136   \n",
       "4    -0.205026  0.013635      -0.029882      -0.14322     -0.219136   \n",
       "...        ...       ...            ...           ...           ...   \n",
       "2071 -0.276900 -0.470936      -0.416541      -0.14322     -0.219136   \n",
       "2072 -0.492522 -0.163156      -0.029882      -0.14322     -0.219136   \n",
       "2073 -0.276900 -1.514995      -1.189860      -0.14322     -0.219136   \n",
       "2074  0.801210  0.037913       1.130096      -0.14322      3.911634   \n",
       "2075 -1.498759  1.262771      -0.029882      -0.14322     -0.219136   \n",
       "\n",
       "      hours-per-week  x0_ Federal-gov  x0_ Local-gov  x0_ Never-worked  \\\n",
       "0          -0.429349              0.0            0.0               0.0   \n",
       "1           0.609927              0.0            1.0               0.0   \n",
       "2           0.769816              0.0            0.0               0.0   \n",
       "3           0.370094              0.0            0.0               0.0   \n",
       "4          -0.189516              0.0            0.0               0.0   \n",
       "...              ...              ...            ...               ...   \n",
       "2071       -0.909015              0.0            0.0               0.0   \n",
       "2072       -0.029627              0.0            0.0               0.0   \n",
       "2073       -2.747736              0.0            0.0               0.0   \n",
       "2074        0.609927              0.0            0.0               0.0   \n",
       "2075       -2.427958              0.0            0.0               0.0   \n",
       "\n",
       "      x0_ Private  ...  x7_ Portugal  x7_ Puerto-Rico  x7_ Scotland  \\\n",
       "0             0.0  ...           0.0              0.0           0.0   \n",
       "1             0.0  ...           0.0              0.0           0.0   \n",
       "2             0.0  ...           0.0              0.0           0.0   \n",
       "3             1.0  ...           0.0              0.0           0.0   \n",
       "4             1.0  ...           0.0              0.0           0.0   \n",
       "...           ...  ...           ...              ...           ...   \n",
       "2071          1.0  ...           0.0              0.0           0.0   \n",
       "2072          1.0  ...           0.0              0.0           0.0   \n",
       "2073          1.0  ...           0.0              0.0           0.0   \n",
       "2074          0.0  ...           0.0              0.0           0.0   \n",
       "2075          1.0  ...           0.0              0.0           0.0   \n",
       "\n",
       "      x7_ South  x7_ Taiwan  x7_ Thailand  x7_ Trinadad&Tobago  \\\n",
       "0           0.0         0.0           0.0                  0.0   \n",
       "1           0.0         0.0           0.0                  0.0   \n",
       "2           0.0         0.0           0.0                  0.0   \n",
       "3           0.0         0.0           0.0                  0.0   \n",
       "4           0.0         0.0           0.0                  0.0   \n",
       "...         ...         ...           ...                  ...   \n",
       "2071        0.0         0.0           0.0                  0.0   \n",
       "2072        0.0         0.0           0.0                  0.0   \n",
       "2073        0.0         0.0           0.0                  0.0   \n",
       "2074        0.0         0.0           0.0                  0.0   \n",
       "2075        0.0         0.0           0.0                  0.0   \n",
       "\n",
       "      x7_ United-States  x7_ Vietnam  x7_ Yugoslavia  \n",
       "0                   1.0          0.0             0.0  \n",
       "1                   1.0          0.0             0.0  \n",
       "2                   1.0          0.0             0.0  \n",
       "3                   1.0          0.0             0.0  \n",
       "4                   1.0          0.0             0.0  \n",
       "...                 ...          ...             ...  \n",
       "2071                1.0          0.0             0.0  \n",
       "2072                1.0          0.0             0.0  \n",
       "2073                1.0          0.0             0.0  \n",
       "2074                1.0          0.0             0.0  \n",
       "2075                1.0          0.0             0.0  \n",
       "\n",
       "[2076 rows x 104 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Let's start with a basic `LogisticRegression` model. Set `solver=liblinear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit on X_t_df\n",
    "logreg.fit(X_t_df, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make some predictions on the training and validation data and calculate the performance metric for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = logreg.predict(X_t_df)\n",
    "val_preds = logreg.predict(X_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision:  0.7359855334538878\n",
      "Validation precision:  0.7566844919786097\n"
     ]
    }
   ],
   "source": [
    "print('Training precision: ', precision_score(y_t, train_preds))\n",
    "print('Validation precision: ', precision_score(y_val, val_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation precision is higher than the training precision. Model is underfit. Let's check the class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12435\n",
       "1     3846\n",
       "Name: salary, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive class is extremely undersampled. Let's use SMOTE to balance this data a little more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTENC(categorical_features=categorical.columns, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-5b7a71b3f71d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0marrays_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         self.sampling_strategy_ = check_sampling_strategy(\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/imblearn/over_sampling/_smote.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \"\"\"\n\u001b[1;32m    903\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicate_one_vs_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    645\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input contains NaN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "X_res, y_res = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
